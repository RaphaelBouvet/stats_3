{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation, Model Selection & Regularisation\n",
    "\n",
    "In this notebook we will introduce the concepts below, and how they can be implemented in `scikit-learn`.\n",
    "* Cross validation - a method for estimating the test error rate when test data is not available\n",
    "* Model selection - how we use cross validation to select which model (from a selection) we should use for a particular data set\n",
    "* Regularisation - an adaptation of linear regression to make it more flexible\n",
    "\n",
    "[This video](https://www.youtube.com/watch?v=DQWI1kvmwRg) describes some of the ideas you will face in the coming notebook. The ideas we are covering here are described much more throughly in **ISLR** (see suggested sections in module overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "We have already seen the notion of splitting *test* and *training sets* in order to asses model performance. Now we will introduce the idea of *validation sets*.\n",
    "\n",
    "Once again **ISTL** (Section 5.1) provides a very clear overview of how these technqiues work:\n",
    "\n",
    ">Resampling methods are an indispensable tool in modern statistics. They\n",
    "involve repeatedly drawing samples from a training set and refitting a model\n",
    "of interest on each sample in order to obtain additional information about\n",
    "the fitted model. For example, in order to estimate the variability of a linear\n",
    "regression fit, we can repeatedly draw different samples from the training\n",
    "data, fit a linear regression to each new sample, and then examine the\n",
    "extent to which the resulting fits differ. Such an approach may allow us to\n",
    "obtain information that would not be available from fitting the model only\n",
    "once using the original training sample.\n",
    "\n",
    ">Resampling approaches can be computationally expensive, because they\n",
    "involve fitting the same statistical method multiple times using different\n",
    "subsets of the training data. However, due to recent advances in computing\n",
    "power, the computational requirements of resampling methods generally\n",
    "are not prohibitive. [...] cross-validation can be used to estimate the test\n",
    "error associated with a given statistical learning method in order to evaluate\n",
    "its performance, or to select the appropriate level of flexibility. The process\n",
    "of evaluating a modelâ€™s performance is known as model assessment, whereas model\n",
    "the process of selecting the proper level of flexibility for a model is known as assessment\n",
    "model selection.\n",
    "\n",
    "The [diagram](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7) below illustrates this process. Here the train and validation sets are used to do model assesment and model selection (NOTE: the test set is strictly forbidden from being used in any way during this process!). Once a model is selected the test set is used to do a final assesment of performance to see if the model selected will generalise as well as predicted.\n",
    "\n",
    "<img src=\"././images/testtrainvalid.png\" width=\"450px\">\n",
    "\n",
    "There is a simple overview these ideas [here](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7), and a more thorough overview [here](https://machinelearningmastery.com/difference-test-validation-datasets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation\n",
    "\n",
    "A very common method called *k-fold* is often used, which actually splits the training set multiple times. This allows us to assess the accuracy of the model over $k$ validation splits of data. The [image below](http://ethen8181.github.io/machine-learning/model_selection/model_selection.html) illustrates how this works for $k = 5$ splits.\n",
    "\n",
    "<img src=\"././images/kfold.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` documentation offers a simple [example](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) of implementing k-fold on some dummy data. We will examine this below. NOTE: The `scikit-learn` documentation is **FANSTASTIC(!)** and contains working examples of every function, it should always be the first place you look when you wish to implement a new function.\n",
    "\n",
    "**Task 1:** \n",
    "\n",
    "* Have a look at the code below and check you understand what is going on. (Add some print statements in various places to help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3] VALID: [0 1]\n",
      "TRAIN: [0 1] VALID: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([['A', 'B'], ['C', 'D'], ['E', 'F'], ['G', 'H']])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2) # here we choose the number of folds (or splits) we will make\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "for train_index, valid_index in kf.split(X): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    print(\"TRAIN:\", train_index, \"VALID:\", valid_index)\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this to any data set to perform operations as it gives us the dataframe/numpy array indicies in each loop to select the appropriate data for each fold. For example we can apply this to the auto data set. This contains rows which correspond to individual cars and their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0  18.0          8         307.0         130    3504          12.0    70   \n",
       "1  15.0          8         350.0         165    3693          11.5    70   \n",
       "2  18.0          8         318.0         150    3436          11.0    70   \n",
       "3  16.0          8         304.0         150    3433          12.0    70   \n",
       "4  17.0          8         302.0         140    3449          10.5    70   \n",
       "\n",
       "   origin                       name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('./data/Auto.csv')\n",
    "auto = auto[auto.horsepower != '?']\n",
    "auto['horsepower'] = auto.horsepower.astype(int)\n",
    "auto.reset_index(inplace=True, drop=True)\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = auto.pop('mpg') # mpg will be our target and so we remove this into a seperate array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we are trying to predict 'mpg' from our other automobile data features. We can use KFold to iterate over the number of splits we choose.\n",
    "\n",
    "**Task 2:**\n",
    "* try adding print statements for the size of the dataframes in each split\n",
    "* try increasing the number of splits and re-run your code\n",
    "* use the code below to print a car name contained in the train and validation data set, for each split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=8, random_state=None, shuffle=False)\n",
      "------------------------------\n",
      "This is split no: 1\n",
      "peugeot 304\n",
      "buick skylark 320\n",
      "------------------------------\n",
      "This is split no: 2\n",
      "buick skylark 320\n",
      "peugeot 304\n",
      "------------------------------\n",
      "This is split no: 3\n",
      "buick skylark 320\n",
      "ford maverick\n",
      "------------------------------\n",
      "This is split no: 4\n",
      "buick skylark 320\n",
      "subaru\n",
      "------------------------------\n",
      "This is split no: 5\n",
      "buick skylark 320\n",
      "dodge aspen se\n",
      "------------------------------\n",
      "This is split no: 6\n",
      "buick skylark 320\n",
      "honda civic cvcc\n",
      "------------------------------\n",
      "This is split no: 7\n",
      "buick skylark 320\n",
      "mercedes benz 300d\n",
      "------------------------------\n",
      "This is split no: 8\n",
      "buick skylark 320\n",
      "toyota tercel\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=8) # here we choose the number of folds (or splits) we will make\n",
    "kf.get_n_splits(auto)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "\n",
    "split_counter = 1\n",
    "for train_index, valid_index in kf.split(auto): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    print('-'*30)\n",
    "    print('This is split no: {}'.format(split_counter))\n",
    "    split_counter += 1 \n",
    "    X_train, X_valid = auto.iloc[train_index], auto.iloc[valid_index] # must use .iloc because its a dataframe this time\n",
    "    y_train, y_valid = mpg[train_index], mpg[valid_index]\n",
    "    \n",
    "    # your code here\n",
    "    print(X_train.name.unique()[1])\n",
    "    print(X_valid.name.unique()[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto.drop('name', axis=1, errors='ignore') # we do not need the car names so remove for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same loop to fit and evaluate our linear regression model on each train/validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "This is split no: 1\n",
      "training MSE: 11.284070590566001\n",
      "validation MSE: 14.974307651304168\n",
      "------------------------------\n",
      "This is split no: 2\n",
      "training MSE: 11.155158050598772\n",
      "validation MSE: 10.905952427081171\n",
      "------------------------------\n",
      "This is split no: 3\n",
      "training MSE: 12.16010513687152\n",
      "validation MSE: 5.991708610108162\n",
      "------------------------------\n",
      "This is split no: 4\n",
      "training MSE: 9.921674145405685\n",
      "validation MSE: 15.587544657621601\n",
      "------------------------------\n",
      "This is split no: 5\n",
      "training MSE: 7.977511689294959\n",
      "validation MSE: 27.844743081984223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "split_counter = 1\n",
    "mse_scores = [] # create empty list of scores for each split\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(auto)\n",
    "\n",
    "for train_index, valid_index in kf.split(auto): # kf.split(X) is an iterable which gives us the indices of the data in each fold\n",
    "    print('-'*30)\n",
    "    print('This is split no: {}'.format(split_counter))\n",
    "    split_counter += 1 \n",
    "    X_train, X_valid = auto.iloc[train_index], auto.iloc[valid_index] # must use .iloc because its a dataframe this time\n",
    "    y_train, y_valid = mpg[train_index], mpg[valid_index]\n",
    "    \n",
    "    \n",
    "    #### fit polynomial to train data in this split\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    \n",
    "    #### eval & print MSE training results in this split\n",
    "    mpg_train_pred = lin_reg.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train, mpg_train_pred)\n",
    "    print('training MSE: {0}'.format(mse_train))\n",
    "    \n",
    "    #### do the same for validation split\n",
    "    mpg_valid_pred = lin_reg.predict(X_valid)\n",
    "    mse_valid = mean_squared_error(y_valid, mpg_valid_pred)\n",
    "    print('validation MSE: {0}'.format(mse_valid))\n",
    "    \n",
    "    mse_scores.append(mse_valid) # assign validation MSE score to list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using k-fold cross validation we can analyse the validation MSE result for each split to assess the overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION SET MSE SCORES\n",
      "mean MSE: 15.060851285619865\n",
      "std MSE: 7.255691814890938\n"
     ]
    }
   ],
   "source": [
    "mse_scores  = np.array(mse_scores)\n",
    "print('VALIDATION SET MSE SCORES')\n",
    "print('mean MSE:', mse_scores.mean())\n",
    "print('std MSE:', mse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation in practice in sklearn\n",
    "Most of the time we do not care about having access to each split. `Scikit-Learn` provide a much easier way to do all of this with the function `cross_val_score`. This allows us to do the same as above but in much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MSE: 15.060851285619865\n",
      "std MSE: 7.255691814890938\n",
      "[-14.97430765 -10.90595243  -5.99170861 -15.58754466 -27.84474308]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "cv_scores = cross_val_score(lin_reg, auto, mpg, cv = 5, scoring='neg_mean_squared_error') # utilisation de neg car convention dans sklearn que higher scoring val is better\n",
    "\n",
    "print('mean MSE:',np.mean(-cv_scores))\n",
    "print('std MSE:',np.std(-cv_scores))\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "\n",
    "* Make sure you understand the output of the cross_val_score above (i.e. What is cv_scores?)\n",
    "* Investigate what the `cross_val_predict` function does.\n",
    "* Import and implement this on the same data as above.\n",
    "* What are the outputs of this function?\n",
    "* Can you use these to evaluate the results of your cross validation?\n",
    "* Do the cross-validation scores give you confidence this model is providing a useful prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross_val_predict\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "cv_predict = cross_val_predict(lin_reg, auto, mpg, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross_val_predict returns the predicted value when the element was in the validation set.  \n",
    "It is not suitable for evaluation because it blends the 5 models together...  \n",
    "It is best used for data viz (or Model blending ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7689592371135645\n",
      "0.9250932680525775\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.sqrt(-cv_scores)))\n",
    "print(np.std(np.sqrt(-cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.805007486571799"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1: wine cross-validation\n",
    "\n",
    "You must predict the alcohol content of various wines based on their other attributes.\n",
    "\n",
    "* Split the data into train and test data sets (Ensure you use the option: `random_state = 28`).\n",
    "* Perform linear regression using k-fold cross validation(ensure you use 5 folds). Return the cross validation MSE errors. Return the mean and standard deviations of these.\n",
    "* Evaluate the performance of the model on the test set.\n",
    "* Compare the cross-validation error and the test error (MSE). What do you find? \n",
    "* Try removing the random_state option. What happens to your results? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your solution here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "wine = pd.read_csv('./data/wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alcohol = wine.pop('Alcohol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grape</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.21</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>1.27</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-1.34</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.42</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>1.35</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.83</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.50</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Grape  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0        1       -0.56  0.23              -1.17       1.91           0.81   \n",
       "1        1       -0.50 -0.83              -2.48       0.02           0.57   \n",
       "2        1        0.02  1.11              -0.27       0.09           0.81   \n",
       "3        1       -0.35  0.49              -0.81       0.93           2.48   \n",
       "4        1        0.23  1.84               0.45       1.28           0.81   \n",
       "..     ...         ...   ...                ...        ...            ...   \n",
       "173      3        2.97  0.30               0.30      -0.33          -0.98   \n",
       "174      3        1.41  0.41               1.05       0.16          -0.79   \n",
       "175      3        1.74 -0.39               0.15       1.42          -1.13   \n",
       "176      3        0.23  0.01               0.15       1.42          -1.03   \n",
       "177      3        1.58  1.36               1.50      -0.26          -0.39   \n",
       "\n",
       "     Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0          1.03                 -0.66             1.22             0.25  0.36   \n",
       "1          0.73                 -0.82            -0.54            -0.29  0.40   \n",
       "2          1.21                 -0.50             2.13             0.27  0.32   \n",
       "3          1.46                 -0.98             1.03             1.18 -0.43   \n",
       "4          0.66                  0.23             0.40            -0.32  0.36   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173       -1.42                  1.27            -0.93             1.14 -1.39   \n",
       "174       -1.28                  0.55            -0.32             0.97 -1.13   \n",
       "175       -1.34                  0.55            -0.42             2.22 -1.61   \n",
       "176       -1.35                  1.35            -0.23             1.83 -1.56   \n",
       "177       -1.27                  1.59            -0.42             1.79 -1.52   \n",
       "\n",
       "     OD280/OD315 of diluted wines  Proline  \n",
       "0                            1.84     1.01  \n",
       "1                            1.11     0.96  \n",
       "2                            0.79     1.39  \n",
       "3                            1.18     2.33  \n",
       "4                            0.45    -0.04  \n",
       "..                            ...      ...  \n",
       "173                         -1.23    -0.02  \n",
       "174                         -1.48     0.01  \n",
       "175                         -1.48     0.28  \n",
       "176                         -1.40     0.30  \n",
       "177                         -1.42    -0.59  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine, alcohol,random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MSE: 0.4771472419568701\n",
      "mean RMSE : 0.6828399536518984\n",
      "std MSE: 0.13726764524713442\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "cv_scores = cross_val_score(lin_reg,X_train,y_train,cv=5,scoring='neg_mean_squared_error')\n",
    "print('mean MSE:',np.mean(-cv_scores))\n",
    "print(f'mean RMSE : {np.mean(np.sqrt(-cv_scores))}')\n",
    "print('std MSE:',np.std(-cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.47723 | RMSE 0.69082\n"
     ]
    }
   ],
   "source": [
    "lin_reg.fit(X_train,y_train)\n",
    "y_test_predict = lin_reg.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test,y_test_predict)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(f'MSE {mse_test:.5f} | RMSE {rmse_test:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean   -0.000112\n",
       "std     0.999626\n",
       "Name: Alcohol, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alcohol.agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularisation\n",
    "\n",
    "An alternative to choosing models which contains smaller numbers of features is to use a method that *constrains* or *regularises* the coefficent estimates assigned to each feature, or that shrinks the coefficient towards zero. This technique is very similar to *least squares* which we have been using until now. Please refer to Section in 6.2 **ISTL** for a fuller explanation of this.\n",
    "\n",
    "When we move to use a regularised linear regression for prediction the additional term means that we now have a model parameter that requires setting. These terms are referred to as *hyperparameters* in machine learning. In practice this introduces another additional unknown parameter which we must choose somewhere in our modelling. It is common practice to run several models, each with different values of this hyperparameter, and then assess the error of each using cross validation for comparison.\n",
    "\n",
    "For now we will focus on how to implement Lasso and Ridge regression in sklearn. These are both types of regularised linear regression.\n",
    "\n",
    "#### Lasso regression in sklearn\n",
    "\n",
    "In this example we aim to predict credit rating of individual customers. To train and predict using a Lasso regression we follow much the same procedure as we have seen before in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources supplÃ©mentaires si vous souhaitez creuser :\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net\n",
    "\n",
    "http://eric.univ-lyon2.fr/~ricco/cours/slides/regularized_regression.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>African American</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Cards  Age  Education  Gender  Student  Married  Balance  \\\n",
       "0   14.891   3606      2   34         11       0        0        1      333   \n",
       "1  106.025   6645      3   82         15       1        1        1      903   \n",
       "2  104.593   7075      4   71         11       0        0        0      580   \n",
       "3  148.924   9504      3   36         11       1        0        0      964   \n",
       "4   55.882   4897      2   68         16       0        0        1      331   \n",
       "\n",
       "   African American  Asian  Caucasian  \n",
       "0                 0      0          1  \n",
       "1                 0      1          0  \n",
       "2                 0      1          0  \n",
       "3                 0      1          0  \n",
       "4                 0      0          1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_csv('./data/credit_modified.csv')\n",
    "rating = credit.pop('Rating')\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### splitting train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(credit, rating, random_state = 91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean MSE: 148.3658024815139\n",
      "std MSE: 22.577610846122578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = 10)\n",
    "cv_scores = cross_val_score(lasso, X_train, y_train, cv = 5, scoring='neg_mean_squared_error')\n",
    "print('mean MSE:',np.mean(-cv_scores))\n",
    "print('std MSE:',np.std(-cv_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can alter the alpha parameter to change the amount of regularisation the model uses (try this yourself! - vary the value by at least factors of 10). With increases in regularisation we expect a reduction in the *variance* of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Lasso with grid search\n",
    "\n",
    "In practice we do not want to vary hyperparameters by hand to find which value is best (the model with minimum cross validation error). Of course `scikit-learn` has a function that automates this for you. Using `GridSearchCV` we pass a dictionary of parameter values we wish to investigate. The function will fit each model we have listed and calculate the cross validation error of each. It provides all the results through the object it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>-134.693531</td>\n",
       "      <td>-105.857049</td>\n",
       "      <td>-101.376944</td>\n",
       "      <td>-87.569097</td>\n",
       "      <td>-78.913304</td>\n",
       "      <td>-76.137420</td>\n",
       "      <td>-109.786540</td>\n",
       "      <td>-103.885686</td>\n",
       "      <td>-106.104721</td>\n",
       "      <td>-113.455931</td>\n",
       "      <td>-101.778022</td>\n",
       "      <td>16.450021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>-134.553201</td>\n",
       "      <td>-105.948212</td>\n",
       "      <td>-101.096381</td>\n",
       "      <td>-87.881139</td>\n",
       "      <td>-78.937345</td>\n",
       "      <td>-76.585584</td>\n",
       "      <td>-109.682491</td>\n",
       "      <td>-103.106018</td>\n",
       "      <td>-106.112572</td>\n",
       "      <td>-113.688788</td>\n",
       "      <td>-101.759173</td>\n",
       "      <td>16.329243</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-133.132694</td>\n",
       "      <td>-106.809300</td>\n",
       "      <td>-98.722275</td>\n",
       "      <td>-91.394863</td>\n",
       "      <td>-79.351692</td>\n",
       "      <td>-78.012317</td>\n",
       "      <td>-108.680986</td>\n",
       "      <td>-96.604537</td>\n",
       "      <td>-106.093953</td>\n",
       "      <td>-115.062267</td>\n",
       "      <td>-101.386489</td>\n",
       "      <td>15.688554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-127.851853</td>\n",
       "      <td>-111.017555</td>\n",
       "      <td>-92.827631</td>\n",
       "      <td>-106.006184</td>\n",
       "      <td>-89.048154</td>\n",
       "      <td>-82.339570</td>\n",
       "      <td>-105.158164</td>\n",
       "      <td>-94.934777</td>\n",
       "      <td>-102.310102</td>\n",
       "      <td>-122.139519</td>\n",
       "      <td>-103.363351</td>\n",
       "      <td>13.605513</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>3</td>\n",
       "      <td>{'alpha': 3}</td>\n",
       "      <td>-132.499698</td>\n",
       "      <td>-117.173048</td>\n",
       "      <td>-106.238114</td>\n",
       "      <td>-112.301661</td>\n",
       "      <td>-94.847286</td>\n",
       "      <td>-76.537019</td>\n",
       "      <td>-102.509314</td>\n",
       "      <td>-98.851062</td>\n",
       "      <td>-105.902336</td>\n",
       "      <td>-131.338637</td>\n",
       "      <td>-107.819817</td>\n",
       "      <td>15.910751</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.003313      0.000420         0.001259        0.000101       0.001   \n",
       "1       0.002407      0.000285         0.001003        0.000078        0.01   \n",
       "2       0.001800      0.000137         0.000935        0.000083         0.1   \n",
       "3       0.001513      0.000019         0.000883        0.000007           1   \n",
       "4       0.001557      0.000121         0.001003        0.000195           3   \n",
       "\n",
       "             params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.001}        -134.693531        -105.857049        -101.376944   \n",
       "1   {'alpha': 0.01}        -134.553201        -105.948212        -101.096381   \n",
       "2    {'alpha': 0.1}        -133.132694        -106.809300         -98.722275   \n",
       "3      {'alpha': 1}        -127.851853        -111.017555         -92.827631   \n",
       "4      {'alpha': 3}        -132.499698        -117.173048        -106.238114   \n",
       "\n",
       "   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0         -87.569097         -78.913304         -76.137420        -109.786540   \n",
       "1         -87.881139         -78.937345         -76.585584        -109.682491   \n",
       "2         -91.394863         -79.351692         -78.012317        -108.680986   \n",
       "3        -106.006184         -89.048154         -82.339570        -105.158164   \n",
       "4        -112.301661         -94.847286         -76.537019        -102.509314   \n",
       "\n",
       "   split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0        -103.885686        -106.104721        -113.455931      -101.778022   \n",
       "1        -103.106018        -106.112572        -113.688788      -101.759173   \n",
       "2         -96.604537        -106.093953        -115.062267      -101.386489   \n",
       "3         -94.934777        -102.310102        -122.139519      -103.363351   \n",
       "4         -98.851062        -105.902336        -131.338637      -107.819817   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0       16.450021                3  \n",
       "1       16.329243                2  \n",
       "2       15.688554                1  \n",
       "3       13.605513                4  \n",
       "4       15.910751                5  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "param_grid = [\n",
    " {'alpha': [0.001, 0.01, 0.1, 1, 3, 10, 100, 1000]},\n",
    " ]\n",
    "\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` also returns a model (with the best hyperparmeter combination it found) which has been fitted one final time to all of the training data. Therefore it is ready to make predictions on the testing set. The model can be accessed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>-134.693531</td>\n",
       "      <td>-105.857049</td>\n",
       "      <td>-101.376944</td>\n",
       "      <td>-87.569097</td>\n",
       "      <td>-78.913304</td>\n",
       "      <td>-76.137420</td>\n",
       "      <td>-109.786540</td>\n",
       "      <td>-103.885686</td>\n",
       "      <td>-106.104721</td>\n",
       "      <td>-113.455931</td>\n",
       "      <td>-101.778022</td>\n",
       "      <td>16.450021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>-134.553201</td>\n",
       "      <td>-105.948212</td>\n",
       "      <td>-101.096381</td>\n",
       "      <td>-87.881139</td>\n",
       "      <td>-78.937345</td>\n",
       "      <td>-76.585584</td>\n",
       "      <td>-109.682491</td>\n",
       "      <td>-103.106018</td>\n",
       "      <td>-106.112572</td>\n",
       "      <td>-113.688788</td>\n",
       "      <td>-101.759173</td>\n",
       "      <td>16.329243</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-133.132694</td>\n",
       "      <td>-106.809300</td>\n",
       "      <td>-98.722275</td>\n",
       "      <td>-91.394863</td>\n",
       "      <td>-79.351692</td>\n",
       "      <td>-78.012317</td>\n",
       "      <td>-108.680986</td>\n",
       "      <td>-96.604537</td>\n",
       "      <td>-106.093953</td>\n",
       "      <td>-115.062267</td>\n",
       "      <td>-101.386489</td>\n",
       "      <td>15.688554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-127.851853</td>\n",
       "      <td>-111.017555</td>\n",
       "      <td>-92.827631</td>\n",
       "      <td>-106.006184</td>\n",
       "      <td>-89.048154</td>\n",
       "      <td>-82.339570</td>\n",
       "      <td>-105.158164</td>\n",
       "      <td>-94.934777</td>\n",
       "      <td>-102.310102</td>\n",
       "      <td>-122.139519</td>\n",
       "      <td>-103.363351</td>\n",
       "      <td>13.605513</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>3</td>\n",
       "      <td>{'alpha': 3}</td>\n",
       "      <td>-132.499698</td>\n",
       "      <td>-117.173048</td>\n",
       "      <td>-106.238114</td>\n",
       "      <td>-112.301661</td>\n",
       "      <td>-94.847286</td>\n",
       "      <td>-76.537019</td>\n",
       "      <td>-102.509314</td>\n",
       "      <td>-98.851062</td>\n",
       "      <td>-105.902336</td>\n",
       "      <td>-131.338637</td>\n",
       "      <td>-107.819817</td>\n",
       "      <td>15.910751</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>-182.681016</td>\n",
       "      <td>-155.361568</td>\n",
       "      <td>-179.884644</td>\n",
       "      <td>-148.842090</td>\n",
       "      <td>-141.841896</td>\n",
       "      <td>-86.351859</td>\n",
       "      <td>-129.718484</td>\n",
       "      <td>-131.176340</td>\n",
       "      <td>-143.631521</td>\n",
       "      <td>-181.271903</td>\n",
       "      <td>-148.076132</td>\n",
       "      <td>28.028584</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>100</td>\n",
       "      <td>{'alpha': 100}</td>\n",
       "      <td>-188.380866</td>\n",
       "      <td>-143.744333</td>\n",
       "      <td>-184.078341</td>\n",
       "      <td>-145.491583</td>\n",
       "      <td>-157.301014</td>\n",
       "      <td>-103.112064</td>\n",
       "      <td>-146.957766</td>\n",
       "      <td>-126.231725</td>\n",
       "      <td>-143.006029</td>\n",
       "      <td>-181.543599</td>\n",
       "      <td>-151.984732</td>\n",
       "      <td>25.542857</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'alpha': 1000}</td>\n",
       "      <td>-188.587767</td>\n",
       "      <td>-146.517162</td>\n",
       "      <td>-180.483375</td>\n",
       "      <td>-143.257202</td>\n",
       "      <td>-158.504388</td>\n",
       "      <td>-103.017203</td>\n",
       "      <td>-150.127246</td>\n",
       "      <td>-121.916885</td>\n",
       "      <td>-144.912869</td>\n",
       "      <td>-185.805136</td>\n",
       "      <td>-152.312923</td>\n",
       "      <td>26.074170</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.003313      0.000420         0.001259        0.000101       0.001   \n",
       "1       0.002407      0.000285         0.001003        0.000078        0.01   \n",
       "2       0.001800      0.000137         0.000935        0.000083         0.1   \n",
       "3       0.001513      0.000019         0.000883        0.000007           1   \n",
       "4       0.001557      0.000121         0.001003        0.000195           3   \n",
       "5       0.001675      0.000654         0.001006        0.000267          10   \n",
       "6       0.001357      0.000019         0.000879        0.000012         100   \n",
       "7       0.001328      0.000028         0.000887        0.000019        1000   \n",
       "\n",
       "             params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.001}        -134.693531        -105.857049        -101.376944   \n",
       "1   {'alpha': 0.01}        -134.553201        -105.948212        -101.096381   \n",
       "2    {'alpha': 0.1}        -133.132694        -106.809300         -98.722275   \n",
       "3      {'alpha': 1}        -127.851853        -111.017555         -92.827631   \n",
       "4      {'alpha': 3}        -132.499698        -117.173048        -106.238114   \n",
       "5     {'alpha': 10}        -182.681016        -155.361568        -179.884644   \n",
       "6    {'alpha': 100}        -188.380866        -143.744333        -184.078341   \n",
       "7   {'alpha': 1000}        -188.587767        -146.517162        -180.483375   \n",
       "\n",
       "   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0         -87.569097         -78.913304         -76.137420        -109.786540   \n",
       "1         -87.881139         -78.937345         -76.585584        -109.682491   \n",
       "2         -91.394863         -79.351692         -78.012317        -108.680986   \n",
       "3        -106.006184         -89.048154         -82.339570        -105.158164   \n",
       "4        -112.301661         -94.847286         -76.537019        -102.509314   \n",
       "5        -148.842090        -141.841896         -86.351859        -129.718484   \n",
       "6        -145.491583        -157.301014        -103.112064        -146.957766   \n",
       "7        -143.257202        -158.504388        -103.017203        -150.127246   \n",
       "\n",
       "   split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0        -103.885686        -106.104721        -113.455931      -101.778022   \n",
       "1        -103.106018        -106.112572        -113.688788      -101.759173   \n",
       "2         -96.604537        -106.093953        -115.062267      -101.386489   \n",
       "3         -94.934777        -102.310102        -122.139519      -103.363351   \n",
       "4         -98.851062        -105.902336        -131.338637      -107.819817   \n",
       "5        -131.176340        -143.631521        -181.271903      -148.076132   \n",
       "6        -126.231725        -143.006029        -181.543599      -151.984732   \n",
       "7        -121.916885        -144.912869        -185.805136      -152.312923   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0       16.450021                3  \n",
       "1       16.329243                2  \n",
       "2       15.688554                1  \n",
       "3       13.605513                4  \n",
       "4       15.910751                5  \n",
       "5       28.028584                6  \n",
       "6       25.542857                7  \n",
       "7       26.074170                8  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:**\n",
    "* How many times will the lasso model be fitted when the GridSearchCV function is called above?\n",
    "\n",
    "<details><summary>Hint</summary><br>\n",
    "Check what the `refit=True` parameter does in GridSearchCV\n",
    "</details>\n",
    "\n",
    "* Look through the columns of the `grid_results` dataframe. Try and understand what the table contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nb of fitted with GridSearchCV = 80 (8*alpha *10fold)\n",
    "in grid resuls =>\n",
    " - Fit times mean/std\n",
    " - mean score time (validations)\n",
    " - Test score for each fold (values, mean,sd and rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Congratulations! You have just done your first model hyperparameter tuning in `scikit-learn`! \n",
    "\n",
    "If we have a dataset for which we are interested in developing a predictive model. We do not know beforehand which model will perform best for this particular data or problem. Therefore, we fit and evaluate a number of different models to our data. The models could also be of varying type as well as flexibility (e.g. random forests, support vector machines, linear regression). We then need to decide which of our models we will choose to use in our final product.\n",
    "\n",
    "As **ISLR** states:\n",
    "> \"we can directly estimate the test error using the validation set and cross-validation methods\n",
    "discussed in Chapter 5. We can compute the validation set error or the\n",
    "cross-validation error for each model under consideration, and then select\n",
    "the model for which the resulting estimated test error is smallest.\"\n",
    "\n",
    "This works as a simple rule, which we will follow for the remainder of this notebook. However in practice the selection can sometimes be a bit more nuanced. Read more detail [here](https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 5: Ridge regression in sklearn\n",
    "\n",
    "Another type of regularised linear model is know as *Ridge regression*.\n",
    "\n",
    "* Repeat the model prediction process above on the credit data but use a ridge regression model.\n",
    "* Try replacing `GridSearchCV` with `RandomizedSearchCV`\n",
    "* How do these functions differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>-134.709207</td>\n",
       "      <td>-105.847146</td>\n",
       "      <td>-101.408319</td>\n",
       "      <td>-87.535423</td>\n",
       "      <td>-78.910802</td>\n",
       "      <td>-76.088537</td>\n",
       "      <td>-109.798419</td>\n",
       "      <td>-103.970825</td>\n",
       "      <td>-106.104415</td>\n",
       "      <td>-113.431271</td>\n",
       "      <td>-101.780436</td>\n",
       "      <td>16.463655</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>-134.708083</td>\n",
       "      <td>-105.847646</td>\n",
       "      <td>-101.405663</td>\n",
       "      <td>-87.537922</td>\n",
       "      <td>-78.910278</td>\n",
       "      <td>-76.089237</td>\n",
       "      <td>-109.797587</td>\n",
       "      <td>-103.951593</td>\n",
       "      <td>-106.104574</td>\n",
       "      <td>-113.432645</td>\n",
       "      <td>-101.778523</td>\n",
       "      <td>16.463002</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-134.696920</td>\n",
       "      <td>-105.852592</td>\n",
       "      <td>-101.379213</td>\n",
       "      <td>-87.562901</td>\n",
       "      <td>-78.905195</td>\n",
       "      <td>-76.096104</td>\n",
       "      <td>-109.789211</td>\n",
       "      <td>-103.762049</td>\n",
       "      <td>-106.106093</td>\n",
       "      <td>-113.446278</td>\n",
       "      <td>-101.759656</td>\n",
       "      <td>16.456623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-134.591854</td>\n",
       "      <td>-105.896229</td>\n",
       "      <td>-101.125522</td>\n",
       "      <td>-87.810393</td>\n",
       "      <td>-78.868133</td>\n",
       "      <td>-76.152928</td>\n",
       "      <td>-109.700721</td>\n",
       "      <td>-102.109819</td>\n",
       "      <td>-106.115508</td>\n",
       "      <td>-113.572360</td>\n",
       "      <td>-101.594347</td>\n",
       "      <td>16.404872</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>3</td>\n",
       "      <td>{'alpha': 3}</td>\n",
       "      <td>-134.391253</td>\n",
       "      <td>-105.965205</td>\n",
       "      <td>-100.621625</td>\n",
       "      <td>-88.340987</td>\n",
       "      <td>-78.849639</td>\n",
       "      <td>-76.222951</td>\n",
       "      <td>-109.483397</td>\n",
       "      <td>-99.544295</td>\n",
       "      <td>-106.108819</td>\n",
       "      <td>-113.805661</td>\n",
       "      <td>-101.333383</td>\n",
       "      <td>16.332863</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>-133.890522</td>\n",
       "      <td>-106.090432</td>\n",
       "      <td>-99.320843</td>\n",
       "      <td>-89.982387</td>\n",
       "      <td>-79.071589</td>\n",
       "      <td>-76.191060</td>\n",
       "      <td>-108.669389</td>\n",
       "      <td>-95.536294</td>\n",
       "      <td>-105.969953</td>\n",
       "      <td>-114.432072</td>\n",
       "      <td>-100.915454</td>\n",
       "      <td>16.192486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>100</td>\n",
       "      <td>{'alpha': 100}</td>\n",
       "      <td>-134.083288</td>\n",
       "      <td>-108.689604</td>\n",
       "      <td>-99.945650</td>\n",
       "      <td>-100.567871</td>\n",
       "      <td>-82.857511</td>\n",
       "      <td>-72.598275</td>\n",
       "      <td>-102.841444</td>\n",
       "      <td>-94.279222</td>\n",
       "      <td>-106.194183</td>\n",
       "      <td>-121.853714</td>\n",
       "      <td>-102.391076</td>\n",
       "      <td>16.678747</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'alpha': 1000}</td>\n",
       "      <td>-159.394337</td>\n",
       "      <td>-130.276230</td>\n",
       "      <td>-139.267945</td>\n",
       "      <td>-127.599179</td>\n",
       "      <td>-104.683784</td>\n",
       "      <td>-68.050042</td>\n",
       "      <td>-105.482490</td>\n",
       "      <td>-112.287832</td>\n",
       "      <td>-123.522302</td>\n",
       "      <td>-154.546831</td>\n",
       "      <td>-122.511097</td>\n",
       "      <td>25.363301</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.002194      0.000290         0.001376        0.000102       0.001   \n",
       "1       0.001663      0.000118         0.001141        0.000063        0.01   \n",
       "2       0.001553      0.000019         0.001107        0.000025         0.1   \n",
       "3       0.001373      0.000094         0.000995        0.000099           1   \n",
       "4       0.001424      0.000146         0.000959        0.000083           3   \n",
       "5       0.001303      0.000015         0.000908        0.000016          10   \n",
       "6       0.001295      0.000013         0.000924        0.000039         100   \n",
       "7       0.001298      0.000018         0.000906        0.000006        1000   \n",
       "\n",
       "             params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.001}        -134.709207        -105.847146        -101.408319   \n",
       "1   {'alpha': 0.01}        -134.708083        -105.847646        -101.405663   \n",
       "2    {'alpha': 0.1}        -134.696920        -105.852592        -101.379213   \n",
       "3      {'alpha': 1}        -134.591854        -105.896229        -101.125522   \n",
       "4      {'alpha': 3}        -134.391253        -105.965205        -100.621625   \n",
       "5     {'alpha': 10}        -133.890522        -106.090432         -99.320843   \n",
       "6    {'alpha': 100}        -134.083288        -108.689604         -99.945650   \n",
       "7   {'alpha': 1000}        -159.394337        -130.276230        -139.267945   \n",
       "\n",
       "   split3_test_score  split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0         -87.535423         -78.910802         -76.088537        -109.798419   \n",
       "1         -87.537922         -78.910278         -76.089237        -109.797587   \n",
       "2         -87.562901         -78.905195         -76.096104        -109.789211   \n",
       "3         -87.810393         -78.868133         -76.152928        -109.700721   \n",
       "4         -88.340987         -78.849639         -76.222951        -109.483397   \n",
       "5         -89.982387         -79.071589         -76.191060        -108.669389   \n",
       "6        -100.567871         -82.857511         -72.598275        -102.841444   \n",
       "7        -127.599179        -104.683784         -68.050042        -105.482490   \n",
       "\n",
       "   split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0        -103.970825        -106.104415        -113.431271      -101.780436   \n",
       "1        -103.951593        -106.104574        -113.432645      -101.778523   \n",
       "2        -103.762049        -106.106093        -113.446278      -101.759656   \n",
       "3        -102.109819        -106.115508        -113.572360      -101.594347   \n",
       "4         -99.544295        -106.108819        -113.805661      -101.333383   \n",
       "5         -95.536294        -105.969953        -114.432072      -100.915454   \n",
       "6         -94.279222        -106.194183        -121.853714      -102.391076   \n",
       "7        -112.287832        -123.522302        -154.546831      -122.511097   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0       16.463655                6  \n",
       "1       16.463002                5  \n",
       "2       16.456623                4  \n",
       "3       16.404872                3  \n",
       "4       16.332863                2  \n",
       "5       16.192486                1  \n",
       "6       16.678747                7  \n",
       "7       25.363301                8  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your solution here\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "ridge = Ridge(max_iter=10000)\n",
    "\n",
    "param_grid = [\n",
    " {'alpha': [0.001, 0.01, 0.1, 1, 3, 10, 100, 1000]},\n",
    " ]\n",
    "grid_search_ridge = GridSearchCV(ridge, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "grid_results = pd.DataFrame(grid_search_ridge.cv_results_)\n",
    "\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>124.18432</td>\n",
       "      <td>{'alpha': 124.18431978438628}</td>\n",
       "      <td>-134.812626</td>\n",
       "      <td>-109.527649</td>\n",
       "      <td>-101.402646</td>\n",
       "      <td>-102.271025</td>\n",
       "      <td>-83.678092</td>\n",
       "      <td>-71.742080</td>\n",
       "      <td>-102.097047</td>\n",
       "      <td>-95.010700</td>\n",
       "      <td>-106.684787</td>\n",
       "      <td>-123.655286</td>\n",
       "      <td>-103.088194</td>\n",
       "      <td>17.067264</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.286565</td>\n",
       "      <td>{'alpha': 0.2865647929318674}</td>\n",
       "      <td>-134.674187</td>\n",
       "      <td>-105.862481</td>\n",
       "      <td>-101.325039</td>\n",
       "      <td>-87.614565</td>\n",
       "      <td>-78.895529</td>\n",
       "      <td>-76.109595</td>\n",
       "      <td>-109.771547</td>\n",
       "      <td>-103.384611</td>\n",
       "      <td>-106.108881</td>\n",
       "      <td>-113.473889</td>\n",
       "      <td>-101.722032</td>\n",
       "      <td>16.444214</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>126.591398</td>\n",
       "      <td>{'alpha': 126.59139752316108}</td>\n",
       "      <td>-134.891113</td>\n",
       "      <td>-109.611208</td>\n",
       "      <td>-101.553724</td>\n",
       "      <td>-102.429497</td>\n",
       "      <td>-83.758335</td>\n",
       "      <td>-71.662533</td>\n",
       "      <td>-102.033560</td>\n",
       "      <td>-95.084569</td>\n",
       "      <td>-106.737521</td>\n",
       "      <td>-123.828959</td>\n",
       "      <td>-103.159102</td>\n",
       "      <td>17.107298</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.950154</td>\n",
       "      <td>{'alpha': 0.9501538387890021}</td>\n",
       "      <td>-134.597389</td>\n",
       "      <td>-105.894062</td>\n",
       "      <td>-101.139088</td>\n",
       "      <td>-87.796811</td>\n",
       "      <td>-78.869603</td>\n",
       "      <td>-76.150286</td>\n",
       "      <td>-109.705820</td>\n",
       "      <td>-102.191139</td>\n",
       "      <td>-106.115234</td>\n",
       "      <td>-113.565809</td>\n",
       "      <td>-101.602524</td>\n",
       "      <td>16.407278</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>5.392922</td>\n",
       "      <td>{'alpha': 5.392921558298756}</td>\n",
       "      <td>-134.192204</td>\n",
       "      <td>-106.018812</td>\n",
       "      <td>-100.106150</td>\n",
       "      <td>-88.937687</td>\n",
       "      <td>-78.894243</td>\n",
       "      <td>-76.244853</td>\n",
       "      <td>-109.206323</td>\n",
       "      <td>-97.616017</td>\n",
       "      <td>-106.072223</td>\n",
       "      <td>-114.038304</td>\n",
       "      <td>-101.132682</td>\n",
       "      <td>16.275856</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>967.343154</td>\n",
       "      <td>{'alpha': 967.343154172192}</td>\n",
       "      <td>-158.855727</td>\n",
       "      <td>-129.806139</td>\n",
       "      <td>-138.485948</td>\n",
       "      <td>-127.124476</td>\n",
       "      <td>-104.174823</td>\n",
       "      <td>-67.945014</td>\n",
       "      <td>-105.229858</td>\n",
       "      <td>-111.919074</td>\n",
       "      <td>-123.137012</td>\n",
       "      <td>-153.983205</td>\n",
       "      <td>-122.066128</td>\n",
       "      <td>25.227055</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>31.244166</td>\n",
       "      <td>{'alpha': 31.244166260525304}</td>\n",
       "      <td>-133.225657</td>\n",
       "      <td>-106.499925</td>\n",
       "      <td>-97.765382</td>\n",
       "      <td>-93.663974</td>\n",
       "      <td>-80.134448</td>\n",
       "      <td>-75.460024</td>\n",
       "      <td>-106.584964</td>\n",
       "      <td>-93.123647</td>\n",
       "      <td>-105.614023</td>\n",
       "      <td>-116.196291</td>\n",
       "      <td>-100.826833</td>\n",
       "      <td>16.018206</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>114.014462</td>\n",
       "      <td>{'alpha': 114.01446224209333}</td>\n",
       "      <td>-134.491718</td>\n",
       "      <td>-109.174694</td>\n",
       "      <td>-100.774634</td>\n",
       "      <td>-101.580904</td>\n",
       "      <td>-83.336560</td>\n",
       "      <td>-72.089500</td>\n",
       "      <td>-102.385419</td>\n",
       "      <td>-94.700071</td>\n",
       "      <td>-106.469054</td>\n",
       "      <td>-122.910403</td>\n",
       "      <td>-102.791296</td>\n",
       "      <td>16.900357</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.52776</td>\n",
       "      <td>{'alpha': 0.5277595174724746}</td>\n",
       "      <td>-134.645571</td>\n",
       "      <td>-105.874581</td>\n",
       "      <td>-101.256268</td>\n",
       "      <td>-87.681097</td>\n",
       "      <td>-78.884660</td>\n",
       "      <td>-76.125636</td>\n",
       "      <td>-109.748149</td>\n",
       "      <td>-102.925449</td>\n",
       "      <td>-106.111805</td>\n",
       "      <td>-113.508374</td>\n",
       "      <td>-101.676159</td>\n",
       "      <td>16.429615</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.920493</td>\n",
       "      <td>{'alpha': 0.9204929606732325}</td>\n",
       "      <td>-134.600697</td>\n",
       "      <td>-105.892761</td>\n",
       "      <td>-101.147186</td>\n",
       "      <td>-87.788721</td>\n",
       "      <td>-78.870508</td>\n",
       "      <td>-76.148688</td>\n",
       "      <td>-109.708843</td>\n",
       "      <td>-102.240037</td>\n",
       "      <td>-106.115059</td>\n",
       "      <td>-113.561889</td>\n",
       "      <td>-101.607439</td>\n",
       "      <td>16.408730</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0        0.001868      0.000181         0.001267        0.000124   124.18432   \n",
       "1        0.001695      0.000074         0.001158        0.000044    0.286565   \n",
       "2        0.001617      0.000084         0.001114        0.000019  126.591398   \n",
       "3        0.001493      0.000136         0.001110        0.000298    0.950154   \n",
       "4        0.001459      0.000258         0.000955        0.000075    5.392922   \n",
       "..            ...           ...              ...             ...         ...   \n",
       "95       0.001362      0.000133         0.000958        0.000140  967.343154   \n",
       "96       0.001293      0.000019         0.000895        0.000005   31.244166   \n",
       "97       0.001282      0.000006         0.000902        0.000017  114.014462   \n",
       "98       0.001282      0.000010         0.000903        0.000015     0.52776   \n",
       "99       0.001302      0.000050         0.000925        0.000084    0.920493   \n",
       "\n",
       "                           params  split0_test_score  split1_test_score  \\\n",
       "0   {'alpha': 124.18431978438628}        -134.812626        -109.527649   \n",
       "1   {'alpha': 0.2865647929318674}        -134.674187        -105.862481   \n",
       "2   {'alpha': 126.59139752316108}        -134.891113        -109.611208   \n",
       "3   {'alpha': 0.9501538387890021}        -134.597389        -105.894062   \n",
       "4    {'alpha': 5.392921558298756}        -134.192204        -106.018812   \n",
       "..                            ...                ...                ...   \n",
       "95    {'alpha': 967.343154172192}        -158.855727        -129.806139   \n",
       "96  {'alpha': 31.244166260525304}        -133.225657        -106.499925   \n",
       "97  {'alpha': 114.01446224209333}        -134.491718        -109.174694   \n",
       "98  {'alpha': 0.5277595174724746}        -134.645571        -105.874581   \n",
       "99  {'alpha': 0.9204929606732325}        -134.600697        -105.892761   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -101.402646        -102.271025         -83.678092   \n",
       "1         -101.325039         -87.614565         -78.895529   \n",
       "2         -101.553724        -102.429497         -83.758335   \n",
       "3         -101.139088         -87.796811         -78.869603   \n",
       "4         -100.106150         -88.937687         -78.894243   \n",
       "..                ...                ...                ...   \n",
       "95        -138.485948        -127.124476        -104.174823   \n",
       "96         -97.765382         -93.663974         -80.134448   \n",
       "97        -100.774634        -101.580904         -83.336560   \n",
       "98        -101.256268         -87.681097         -78.884660   \n",
       "99        -101.147186         -87.788721         -78.870508   \n",
       "\n",
       "    split5_test_score  split6_test_score  split7_test_score  \\\n",
       "0          -71.742080        -102.097047         -95.010700   \n",
       "1          -76.109595        -109.771547        -103.384611   \n",
       "2          -71.662533        -102.033560         -95.084569   \n",
       "3          -76.150286        -109.705820        -102.191139   \n",
       "4          -76.244853        -109.206323         -97.616017   \n",
       "..                ...                ...                ...   \n",
       "95         -67.945014        -105.229858        -111.919074   \n",
       "96         -75.460024        -106.584964         -93.123647   \n",
       "97         -72.089500        -102.385419         -94.700071   \n",
       "98         -76.125636        -109.748149        -102.925449   \n",
       "99         -76.148688        -109.708843        -102.240037   \n",
       "\n",
       "    split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n",
       "0         -106.684787        -123.655286      -103.088194       17.067264   \n",
       "1         -106.108881        -113.473889      -101.722032       16.444214   \n",
       "2         -106.737521        -123.828959      -103.159102       17.107298   \n",
       "3         -106.115234        -113.565809      -101.602524       16.407278   \n",
       "4         -106.072223        -114.038304      -101.132682       16.275856   \n",
       "..                ...                ...              ...             ...   \n",
       "95        -123.137012        -153.983205      -122.066128       25.227055   \n",
       "96        -105.614023        -116.196291      -100.826833       16.018206   \n",
       "97        -106.469054        -122.910403      -102.791296       16.900357   \n",
       "98        -106.111805        -113.508374      -101.676159       16.429615   \n",
       "99        -106.115059        -113.561889      -101.607439       16.408730   \n",
       "\n",
       "    rank_test_score  \n",
       "0                80  \n",
       "1                64  \n",
       "2                81  \n",
       "3                50  \n",
       "4                26  \n",
       "..              ...  \n",
       "95              100  \n",
       "96                9  \n",
       "97               79  \n",
       "98               56  \n",
       "99               51  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.fixes import loguniform\n",
    "ridge = Ridge(max_iter=10000)\n",
    "\n",
    "param_grid = [\n",
    " {'alpha': loguniform(0.1,1000)},\n",
    " ]\n",
    "grid_search_ridge = RandomizedSearchCV(ridge,param_grid,cv=10,n_iter=100, scoring='neg_mean_squared_error')\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "grid_results = pd.DataFrame(grid_search_ridge.cv_results_)\n",
    "\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_fit_time                            0.001296\n",
       "std_fit_time                             0.000022\n",
       "mean_score_time                          0.000912\n",
       "std_score_time                           0.000021\n",
       "param_alpha                             20.125308\n",
       "params               {'alpha': 20.12530845519356}\n",
       "split0_test_score                     -133.461995\n",
       "split1_test_score                     -106.253666\n",
       "split2_test_score                      -98.262553\n",
       "split3_test_score                      -91.920546\n",
       "split4_test_score                      -79.577756\n",
       "split5_test_score                      -75.892632\n",
       "split6_test_score                     -107.588994\n",
       "split7_test_score                      -93.721042\n",
       "split8_test_score                     -105.755754\n",
       "split9_test_score                     -115.261122\n",
       "mean_test_score                       -100.769606\n",
       "std_test_score                          16.070061\n",
       "rank_test_score                                 1\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results.loc[grid_results.rank_test_score.idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2: Moneyball\n",
    "\n",
    "Moneyball, as well as being a fantastic story, is also a true story of statistical methods being applied in a real world context to make predictions for decision making. [The film Moneyball](https://www.youtube.com/watch?v=-4QPVo0UIzc) is well worth a watch if you have time. As well as in baseball most major competitive sports teams are now using data science to improve their performance, e.g. [football](http://outsideoftheboot.com/2013/06/26/rise-of-data-analysis-in-football/),...\n",
    "\n",
    "In this exercise you have been hired by Oakland Athletics general manager Billy Beane. Your first mission is to predict the salary each player will make based on other information that is available. This will allow Billy to understand what price he should pay for players in the next transfer season.\n",
    "\n",
    "You must:\n",
    "* Import and prepare the data\n",
    "* Create a train and test set\n",
    "* Implement a regularised model of your choice\n",
    "* Choose optimal parameters for your regularised model\n",
    "* Estimate test-error using k-fold cross validation\n",
    "* Calculate the true test-error\n",
    "\n",
    "<details><summary>HINT 1</summary><br>\n",
    "Some values are missing. You can drop these rows.\n",
    "</details>\n",
    "\n",
    "<details><summary>HINT 2</summary><br>\n",
    "Some columns do not contain numerical values. You can drop these columns.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### note data is in the Hitters.csv file\n",
    "baseball = pd.read_csv('data/Hitters.csv')\n",
    "#### your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0  AtBat  Hits   HmRun  Runs   RBI    Walks  Years  CAtBat  CHits  CHmRun  CRuns  CRBI   CWalks  League  Division  PutOuts  Assists  Errors  Salary  NewLeague\n",
       "False       False  False  False  False  False  False  False  False   False  False   False  False  False   False   False     False    False    False   False   False        263\n",
       "                                                                                                                                                      True    False         59\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>...</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Andy Allanson</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Alan Ashby</td>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Alvin Davis</td>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-Andre Dawson</td>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Andres Galarraga</td>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-Willie McGee</td>\n",
       "      <td>497</td>\n",
       "      <td>127</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>2703</td>\n",
       "      <td>806</td>\n",
       "      <td>...</td>\n",
       "      <td>379</td>\n",
       "      <td>311</td>\n",
       "      <td>138</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>325</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>700.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-Willie Randolph</td>\n",
       "      <td>492</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>50</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>5511</td>\n",
       "      <td>1511</td>\n",
       "      <td>...</td>\n",
       "      <td>897</td>\n",
       "      <td>451</td>\n",
       "      <td>875</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>313</td>\n",
       "      <td>381</td>\n",
       "      <td>20</td>\n",
       "      <td>875.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-Wayne Tolleson</td>\n",
       "      <td>475</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>1700</td>\n",
       "      <td>433</td>\n",
       "      <td>...</td>\n",
       "      <td>217</td>\n",
       "      <td>93</td>\n",
       "      <td>146</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>37</td>\n",
       "      <td>113</td>\n",
       "      <td>7</td>\n",
       "      <td>385.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-Willie Upshaw</td>\n",
       "      <td>573</td>\n",
       "      <td>144</td>\n",
       "      <td>9</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>3198</td>\n",
       "      <td>857</td>\n",
       "      <td>...</td>\n",
       "      <td>470</td>\n",
       "      <td>420</td>\n",
       "      <td>332</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>1314</td>\n",
       "      <td>131</td>\n",
       "      <td>12</td>\n",
       "      <td>960.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>-Willie Wilson</td>\n",
       "      <td>631</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>4908</td>\n",
       "      <td>1457</td>\n",
       "      <td>...</td>\n",
       "      <td>775</td>\n",
       "      <td>357</td>\n",
       "      <td>249</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>408</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
       "0       -Andy Allanson    293    66      1    30   29     14      1     293   \n",
       "1          -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
       "2         -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
       "3        -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
       "4    -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
       "..                 ...    ...   ...    ...   ...  ...    ...    ...     ...   \n",
       "317      -Willie McGee    497   127      7    65   48     37      5    2703   \n",
       "318   -Willie Randolph    492   136      5    76   50     94     12    5511   \n",
       "319    -Wayne Tolleson    475   126      3    61   43     52      6    1700   \n",
       "320     -Willie Upshaw    573   144      9    85   60     78      8    3198   \n",
       "321     -Willie Wilson    631   170      9    77   44     31     11    4908   \n",
       "\n",
       "     CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  \\\n",
       "0       66  ...     30    29      14       A        E     446       33   \n",
       "1      835  ...    321   414     375       N        W     632       43   \n",
       "2      457  ...    224   266     263       A        W     880       82   \n",
       "3     1575  ...    828   838     354       N        E     200       11   \n",
       "4      101  ...     48    46      33       N        E     805       40   \n",
       "..     ...  ...    ...   ...     ...     ...      ...     ...      ...   \n",
       "317    806  ...    379   311     138       N        E     325        9   \n",
       "318   1511  ...    897   451     875       A        E     313      381   \n",
       "319    433  ...    217    93     146       A        W      37      113   \n",
       "320    857  ...    470   420     332       A        E    1314      131   \n",
       "321   1457  ...    775   357     249       A        W     408        4   \n",
       "\n",
       "     Errors  Salary  NewLeague  \n",
       "0        20     NaN          A  \n",
       "1        10   475.0          N  \n",
       "2        14   480.0          A  \n",
       "3         3   500.0          N  \n",
       "4         4    91.5          N  \n",
       "..      ...     ...        ...  \n",
       "317       3   700.0          N  \n",
       "318      20   875.0          A  \n",
       "319       7   385.0          A  \n",
       "320      12   960.0          A  \n",
       "321       3  1000.0          A  \n",
       "\n",
       "[322 rows x 21 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    263\n",
       "True      59\n",
       "Name: Salary, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball.Salary.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = baseball.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "League  Division  NewLeague\n",
       "A       W         A            67\n",
       "        E         A            64\n",
       "N       W         N            58\n",
       "        E         N            56\n",
       "                  A             5\n",
       "        W         A             5\n",
       "A       E         N             4\n",
       "        W         N             4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseball[['League','Division','NewLeague']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=>https://en.wikipedia.org/wiki/Major_League_Baseball  \n",
    "League :  - A = American League\n",
    "          - N = National League  \n",
    "Divition = East, Central, West  \n",
    "Je vais drop ces variables pour l'instantces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_dp = baseball.drop(['Unnamed: 0','League','Division','NewLeague'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseball_dp.hist(bins=50, figsize=(20,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generation train/test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = baseball.pop('Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(credit, rating, random_state = 91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_train,baseball_test,salary_train,salary_test = train_test_split(baseball_dp,salary,random_state=2708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>278.72267</td>\n",
       "      <td>{'alpha': 278.7226699100683}</td>\n",
       "      <td>-0.611816</td>\n",
       "      <td>-0.600478</td>\n",
       "      <td>-0.343642</td>\n",
       "      <td>-0.269018</td>\n",
       "      <td>-0.260673</td>\n",
       "      <td>-0.175962</td>\n",
       "      <td>-0.981985</td>\n",
       "      <td>-0.946016</td>\n",
       "      <td>-0.259924</td>\n",
       "      <td>-0.158769</td>\n",
       "      <td>-0.460828</td>\n",
       "      <td>0.292001</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>913.970982</td>\n",
       "      <td>{'alpha': 913.9709816255289}</td>\n",
       "      <td>-6.594152</td>\n",
       "      <td>-6.530344</td>\n",
       "      <td>-3.711744</td>\n",
       "      <td>-2.897304</td>\n",
       "      <td>-2.801031</td>\n",
       "      <td>-1.926930</td>\n",
       "      <td>-10.574655</td>\n",
       "      <td>-10.244809</td>\n",
       "      <td>-2.856778</td>\n",
       "      <td>-1.696553</td>\n",
       "      <td>-4.983430</td>\n",
       "      <td>3.151692</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>138.525242</td>\n",
       "      <td>{'alpha': 138.52524178782093}</td>\n",
       "      <td>-0.149957</td>\n",
       "      <td>-0.147173</td>\n",
       "      <td>-0.083994</td>\n",
       "      <td>-0.066345</td>\n",
       "      <td>-0.064501</td>\n",
       "      <td>-0.043351</td>\n",
       "      <td>-0.240747</td>\n",
       "      <td>-0.229163</td>\n",
       "      <td>-0.061577</td>\n",
       "      <td>-0.039332</td>\n",
       "      <td>-0.112614</td>\n",
       "      <td>0.071160</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>1.302837</td>\n",
       "      <td>{'alpha': 1.3028365938302051}</td>\n",
       "      <td>-0.508508</td>\n",
       "      <td>-0.414056</td>\n",
       "      <td>-0.130259</td>\n",
       "      <td>-0.143250</td>\n",
       "      <td>-0.395510</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>-0.290299</td>\n",
       "      <td>-0.002897</td>\n",
       "      <td>-0.206475</td>\n",
       "      <td>-0.498139</td>\n",
       "      <td>-0.259122</td>\n",
       "      <td>0.180986</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>739.378264</td>\n",
       "      <td>{'alpha': 739.3782644289752}</td>\n",
       "      <td>-4.316469</td>\n",
       "      <td>-4.260335</td>\n",
       "      <td>-2.425979</td>\n",
       "      <td>-1.895002</td>\n",
       "      <td>-1.833046</td>\n",
       "      <td>-1.261688</td>\n",
       "      <td>-6.921751</td>\n",
       "      <td>-6.677506</td>\n",
       "      <td>-1.869584</td>\n",
       "      <td>-1.110431</td>\n",
       "      <td>-3.257179</td>\n",
       "      <td>2.057830</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>40.176247</td>\n",
       "      <td>{'alpha': 40.17624747006059}</td>\n",
       "      <td>-0.012257</td>\n",
       "      <td>-0.010321</td>\n",
       "      <td>-0.006772</td>\n",
       "      <td>-0.005527</td>\n",
       "      <td>-0.005498</td>\n",
       "      <td>-0.003269</td>\n",
       "      <td>-0.019512</td>\n",
       "      <td>-0.015741</td>\n",
       "      <td>-0.003815</td>\n",
       "      <td>-0.003492</td>\n",
       "      <td>-0.008620</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.204378</td>\n",
       "      <td>{'alpha': 1.2043782746996272}</td>\n",
       "      <td>-0.491036</td>\n",
       "      <td>-0.391836</td>\n",
       "      <td>-0.123313</td>\n",
       "      <td>-0.134641</td>\n",
       "      <td>-0.369283</td>\n",
       "      <td>-0.001712</td>\n",
       "      <td>-0.274861</td>\n",
       "      <td>-0.003105</td>\n",
       "      <td>-0.197776</td>\n",
       "      <td>-0.486850</td>\n",
       "      <td>-0.247442</td>\n",
       "      <td>0.174294</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>34.431714</td>\n",
       "      <td>{'alpha': 34.431714007291156}</td>\n",
       "      <td>-0.009043</td>\n",
       "      <td>-0.008250</td>\n",
       "      <td>-0.004753</td>\n",
       "      <td>-0.004039</td>\n",
       "      <td>-0.004504</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>-0.014331</td>\n",
       "      <td>-0.011561</td>\n",
       "      <td>-0.003133</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.006470</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.524882</td>\n",
       "      <td>{'alpha': 1.5248817034446422}</td>\n",
       "      <td>-0.318864</td>\n",
       "      <td>-0.444644</td>\n",
       "      <td>-0.140824</td>\n",
       "      <td>-0.149975</td>\n",
       "      <td>-0.344818</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>-0.323994</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>-0.222162</td>\n",
       "      <td>-0.571053</td>\n",
       "      <td>-0.252163</td>\n",
       "      <td>0.174696</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>25.354459</td>\n",
       "      <td>{'alpha': 25.35445872042711}</td>\n",
       "      <td>-0.019304</td>\n",
       "      <td>-0.003760</td>\n",
       "      <td>-0.005098</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.005665</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0        0.001937      0.000357         0.001279        0.000249   278.72267   \n",
       "1        0.001648      0.000377         0.001030        0.000089  913.970982   \n",
       "2        0.001444      0.000024         0.000937        0.000020  138.525242   \n",
       "3        0.001785      0.000276         0.001006        0.000192    1.302837   \n",
       "4        0.001475      0.000079         0.000966        0.000039  739.378264   \n",
       "..            ...           ...              ...             ...         ...   \n",
       "95       0.001529      0.000088         0.000942        0.000078   40.176247   \n",
       "96       0.001626      0.000072         0.000903        0.000004    1.204378   \n",
       "97       0.001453      0.000023         0.000912        0.000018   34.431714   \n",
       "98       0.001597      0.000063         0.000912        0.000026    1.524882   \n",
       "99       0.001479      0.000020         0.000900        0.000003   25.354459   \n",
       "\n",
       "                           params  split0_test_score  split1_test_score  \\\n",
       "0    {'alpha': 278.7226699100683}          -0.611816          -0.600478   \n",
       "1    {'alpha': 913.9709816255289}          -6.594152          -6.530344   \n",
       "2   {'alpha': 138.52524178782093}          -0.149957          -0.147173   \n",
       "3   {'alpha': 1.3028365938302051}          -0.508508          -0.414056   \n",
       "4    {'alpha': 739.3782644289752}          -4.316469          -4.260335   \n",
       "..                            ...                ...                ...   \n",
       "95   {'alpha': 40.17624747006059}          -0.012257          -0.010321   \n",
       "96  {'alpha': 1.2043782746996272}          -0.491036          -0.391836   \n",
       "97  {'alpha': 34.431714007291156}          -0.009043          -0.008250   \n",
       "98  {'alpha': 1.5248817034446422}          -0.318864          -0.444644   \n",
       "99   {'alpha': 25.35445872042711}          -0.019304          -0.003760   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           -0.343642          -0.269018          -0.260673   \n",
       "1           -3.711744          -2.897304          -2.801031   \n",
       "2           -0.083994          -0.066345          -0.064501   \n",
       "3           -0.130259          -0.143250          -0.395510   \n",
       "4           -2.425979          -1.895002          -1.833046   \n",
       "..                ...                ...                ...   \n",
       "95          -0.006772          -0.005527          -0.005498   \n",
       "96          -0.123313          -0.134641          -0.369283   \n",
       "97          -0.004753          -0.004039          -0.004504   \n",
       "98          -0.140824          -0.149975          -0.344818   \n",
       "99          -0.005098          -0.004588          -0.004215   \n",
       "\n",
       "    split5_test_score  split6_test_score  split7_test_score  \\\n",
       "0           -0.175962          -0.981985          -0.946016   \n",
       "1           -1.926930         -10.574655         -10.244809   \n",
       "2           -0.043351          -0.240747          -0.229163   \n",
       "3           -0.001823          -0.290299          -0.002897   \n",
       "4           -1.261688          -6.921751          -6.677506   \n",
       "..                ...                ...                ...   \n",
       "95          -0.003269          -0.019512          -0.015741   \n",
       "96          -0.001712          -0.274861          -0.003105   \n",
       "97          -0.002412          -0.014331          -0.011561   \n",
       "98          -0.002416          -0.323994          -0.002882   \n",
       "99          -0.002800          -0.007516          -0.009007   \n",
       "\n",
       "    split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n",
       "0           -0.259924          -0.158769        -0.460828        0.292001   \n",
       "1           -2.856778          -1.696553        -4.983430        3.151692   \n",
       "2           -0.061577          -0.039332        -0.112614        0.071160   \n",
       "3           -0.206475          -0.498139        -0.259122        0.180986   \n",
       "4           -1.869584          -1.110431        -3.257179        2.057830   \n",
       "..                ...                ...              ...             ...   \n",
       "95          -0.003815          -0.003492        -0.008620        0.005350   \n",
       "96          -0.197776          -0.486850        -0.247442        0.174294   \n",
       "97          -0.003133          -0.002674        -0.006470        0.003902   \n",
       "98          -0.222162          -0.571053        -0.252163        0.174696   \n",
       "99          -0.004349          -0.005665        -0.006630        0.004566   \n",
       "\n",
       "    rank_test_score  \n",
       "0                88  \n",
       "1               100  \n",
       "2                61  \n",
       "3                82  \n",
       "4                97  \n",
       "..              ...  \n",
       "95                9  \n",
       "96               80  \n",
       "97                2  \n",
       "98               81  \n",
       "99                3  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(max_iter=1000)\n",
    "param_grid = [\n",
    " {'alpha': loguniform(0.1,1000)},\n",
    " ]\n",
    "grid_search_rdm_lasso = RandomizedSearchCV(lasso,param_grid,cv=10,n_iter=100, scoring='neg_mean_squared_error')\n",
    "grid_search_rdm_lasso.fit(baseball_train, salary_train)\n",
    "\n",
    "grid_results = pd.DataFrame(grid_search_rdm_lasso.cv_results_)\n",
    "\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_fit_time                            0.001494\n",
       "std_fit_time                              0.00005\n",
       "mean_score_time                          0.000914\n",
       "std_score_time                           0.000022\n",
       "param_alpha                             25.842186\n",
       "params               {'alpha': 25.84218558980374}\n",
       "split0_test_score                       -0.017232\n",
       "split1_test_score                       -0.003916\n",
       "split2_test_score                       -0.004427\n",
       "split3_test_score                       -0.004372\n",
       "split4_test_score                       -0.003629\n",
       "split5_test_score                       -0.002361\n",
       "split6_test_score                       -0.007787\n",
       "split7_test_score                       -0.007607\n",
       "split8_test_score                       -0.002853\n",
       "split9_test_score                       -0.004741\n",
       "mean_test_score                         -0.005892\n",
       "std_test_score                           0.004143\n",
       "rank_test_score                                 1\n",
       "Name: 47, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results.loc[grid_results.rank_test_score.idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = grid_results.loc[grid_results.rank_test_score.idxmin()]['param_alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:**\n",
    "* Ridge regression works best when the input variables are standardised. (See section 6.2 **ISLR** for more details.). Try standardising your data before running your model. Do you find different results?\n",
    "* does this model outperform a simple linear regression model?\n",
    "* Which variables are most important in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stats_3]",
   "language": "python",
   "name": "conda-env-stats_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
